{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from nlpaug.augmenter.word import ContextualWordEmbsAug\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      tweet off_label hs_label\n",
      "count                  8887      8887     8887\n",
      "unique                 8887         2        6\n",
      "top      ردينا ع التطنز 😏👊🏻   NOT_OFF   NOT_HS\n",
      "freq                      1      5715     7928\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>off_label</th>\n",
       "      <th>hs_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ردينا ع التطنز 😏👊🏻</td>\n",
       "      <td>OFF</td>\n",
       "      <td>NOT_HS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>وصارت فطاير البقالات غذاء صحي 👎🏻</td>\n",
       "      <td>NOT_OFF</td>\n",
       "      <td>NOT_HS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>روحي لبريده تلقين اشباه كثير بس ماحد زيكم مشف...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>NOT_HS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>مش باين حاجه خالص 😣مش عارف بقى 😔</td>\n",
       "      <td>NOT_OFF</td>\n",
       "      <td>NOT_HS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#اليوم_الاثنين👏 يقولك :%90  من المسلمين عندهم ...</td>\n",
       "      <td>NOT_OFF</td>\n",
       "      <td>NOT_HS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet off_label hs_label\n",
       "0                                 ردينا ع التطنز 😏👊🏻       OFF   NOT_HS\n",
       "1                  وصارت فطاير البقالات غذاء صحي 👎🏻    NOT_OFF   NOT_HS\n",
       "2   روحي لبريده تلقين اشباه كثير بس ماحد زيكم مشف...       OFF   NOT_HS\n",
       "3                   مش باين حاجه خالص 😣مش عارف بقى 😔   NOT_OFF   NOT_HS\n",
       "4  #اليوم_الاثنين👏 يقولك :%90  من المسلمين عندهم ...   NOT_OFF   NOT_HS"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_df = pd.read_csv('./dataset/OSACT2022-sharedTask-train.csv', usecols=['tweet', 'off_label', 'hs_label'])\n",
    "print(train_dataset_df.describe())\n",
    "train_dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT_OFF    5715\n",
      "OFF        3172\n",
      "Name: off_label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='off_label', ylabel='count'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_dataset_df['off_label'].value_counts())\n",
    "sns.countplot(data = train_dataset_df , x='off_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT_HS    7928\n",
      "HS         959\n",
      "Name: hs_label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='hs_label', ylabel='count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "train_dataset_df.loc[train_dataset_df['hs_label'] == 'HS1', 'hs_label'] = 'HS'\n",
    "train_dataset_df.loc[train_dataset_df['hs_label'] == 'HS2', 'hs_label'] = 'HS'\n",
    "train_dataset_df.loc[train_dataset_df['hs_label'] == 'HS3', 'hs_label'] = 'HS'\n",
    "train_dataset_df.loc[train_dataset_df['hs_label'] == 'HS4', 'hs_label'] = 'HS'\n",
    "train_dataset_df.loc[train_dataset_df['hs_label'] == 'HS5', 'hs_label'] = 'HS'\n",
    "train_dataset_df.loc[train_dataset_df['hs_label'] == 'HS6', 'hs_label'] = 'HS'\n",
    "train_dataset_df.loc[(train_dataset_df['hs_label'] == 'NOT_HS'), 'hs_label'] = 'NOT_HS'\n",
    "print(train_dataset_df['hs_label'].value_counts())\n",
    "sns.countplot(data = train_dataset_df , x='hs_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Augmentator\n",
    "TOPK=20 #default=100\n",
    "ACT = 'insert' #\"substitute\"\n",
    "\n",
    "aug_bert = ContextualWordEmbsAug(\n",
    "    model_path= 'UBC-NLP/MARBERT',\n",
    "#     model_path='distilbert-base-uncased', \n",
    "    # device='cuda',\n",
    "    action=ACT, top_k=TOPK)\n",
    "\n",
    "def augment_text(df, augmenter, label_name, label_val, samples=100, pr=0.2, show = 0):\n",
    "    augmenter.aug_p = pr\n",
    "    new_text=[]\n",
    "    \n",
    "    # selecting the minority class samples\n",
    "    df_n = df[df[label_name]==label_val].reset_index(drop=True)\n",
    "\n",
    "    # data augmentation loop\n",
    "    for i in tqdm(np.random.randint(0, len(df_n), samples)):\n",
    "            text = df_n.iloc[i]['tweet']\n",
    "            augmented_text = augmenter.augment(text)\n",
    "            if show:\n",
    "                print(f\"The original text: {text}\")\n",
    "                print(f\"The augmented text: {augmented_text}\")\n",
    "                print('-'*100)\n",
    "            new_text.append(augmented_text)\n",
    "    \n",
    "    # dataframe\n",
    "    new = pd.DataFrame({'tweet':new_text,label_name:label_val})\n",
    "    df = shuffle(pd.concat([df,new]).reset_index(drop=True))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:00,  7.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original text: الريس يخصص ٢٧٥ مليار جنيه ل تنميه شامله ل سيناء ....علشان اسرائيل تأخذ سيناء سوبر لوكس علي مفتاااح 🐑🐏\n",
      "The augmented text: يعني الريس لازم يخصص ٢٧٥ مليار جنيه ل مشروع تنميه شامله شامله ل شبه سيناء.... علشان اسراييل تاخذ مشروع سيناء سوبر لوكس علي مفتاااح [UNK]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The original text: بنات الشعبه عندهم وسواس قهري في المواد يسألون عن تفاصيل التفاصيل 😷😷😷خصوصا وحده تحب توسوس نفسي اصفقها\n",
      "The augmented text: بنات الشعبه عندهم وسواس و قهري في المواد ولازم يسالون عن تفاصيل التفاصيل 😷😷😷خصوصا مع وحده تحب ولا توسوس 😂😂 نفسي اصفقها\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  9.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original text:  نعم فعلاً اقولها من تجربة ومن خلال رؤيتي بأن المعلمين بحاجة مستمرة لدورات تثقيفية يعني مو معقولة كل المعلمات اللي بالمدرسة تحسهم مش متعلمات من طريقة كلامهم وتعاملهم مع الطالبات شيء فظيع والله😣\n",
      "The augmented text: اي نعم فعلا اقولها من تجربة ومن خلال رويتي بان المعلمين بحاجة وبشكل مستمرة لدورات تثقيفية يعني مو معقولة لو كل المعلمات زي اللي عندنا بالمدرسة تحسهم حتى مش متعلمات من طريقة كلامهم وتعاملهم السلبي مع الطالبات شيء جد فظيع والله😣\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The original text: ابغى الي قصة شعره وسوت فيه كذا وينها الكلبه وينها حسبي عليك مع هالصباح😣💔 \n",
      "The augmented text: بموت ابغى الي عجبه قصة شعره وسوت تسوي فيه كذا وينها الكلبه وينها الكلبه حسبي عليك مع هالصباح😣💔\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The original text: اما بشوف واحده مطلعه شعرايه واحده بره الطرحه ببقه عايز اتف في وشها😡😡لاما تبقي قده لاما تقلعيه خالص😡😡\n",
      "The augmented text: اما بشوف صور واحده بشوفها مطلعه شعرايه واحده بره بتاعت الطرحه انا ببقه عايز اتف وشه في وشها😡😡لاما تبقي قده لاما تقلعيه خالص😡😡\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#show sample of augmented offensive tweets\n",
    "aug_df = augment_text(train_dataset_df, aug_bert, 'off_label', label_val ='OFF', samples=5, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:00<00:00, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original text:  ودي يمسكونك انت وامثاله ويخلونككم عبره لكل معتبر فاقد للرجوله 😎ماتقوم رجولتكم الا ع انثى بس 👎🏻\n",
      "The augmented text: ودي بعد يمسكونك انت وامثاله ويخلونككم عبره لكل شخص معتبر ، فاقد الضمير للرجوله 😎ماتقوم رجولتكم الا بالضرب ع انثى بس 👎🏻\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The original text:   في أوروبا تمر عليك البنت مع قليل من المكياج. أما عندنا يمر عليك المكياج مع قليل من البنت 🌚🔪😂😂\n",
      "The augmented text: في اوروبا تمر عليك مكياج البنت مع قليل من المكياج. اما حنا عندنا عادي يمر عليك المكياج كامل مع شوية قليل من البنت 🌚🔪😂😂\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The original text:  بتعملوا ف ديك أبونا كده ليه هو إحنا كفرة ولايهود... وربنا إحنا ماشيين بالعلاج 😡😡😡😡\n",
      "The augmented text: بتعملوا ايه ف ابن ديك ابونا ياجماعة كده ليه هو احنا كفرة ولايهود... وربنا احنا ماشيين بالعلاج اهو 😡😡😡😡\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  9.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original text:  حلوووب جدا جدا جدا لعيون ايفاانكا ٤٨٠ مليار دولار عقبها خلوووك تدفع ضرائب ياا 🐑🐑\n",
      "The augmented text: قوول حلوووب جدا جدا جدا لعيون ايفاانكا ٤٨٠ مليار دولار عقبها قولوا خلوووك تدفع اول ضرايب علينا ياا حمار 🐑🐑\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The original text: اقسم ب الله ناس أصبحت مقرفة برمي الأسامي و التهم ع كل من كان له فكر حر .. يشتو مطبلين و قطيع .. انقلعو الله يقرفكم فاض الكيل بنا منكم !!! 😠😷\n",
      "The augmented text: اقسم ب غضب الله ناس اصبحت مقرفة حتى برمي الاسامي و التهم ع كل من هو كان له فكر او حر.. يشتو مطبلين حمير و قطيع.. انقلعو الله لا يقرفكم فاض الكيل بنا الكيل منكم!!! 😒 😠😷\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#show sample of augmented hate speech tweets\n",
    "aug_df = augment_text(train_dataset_df, aug_bert, 'hs_label', label_val ='HS', samples=5, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2543/2543 [17:44<00:00,  2.39it/s]\n"
     ]
    }
   ],
   "source": [
    "#augment offensive tweets\n",
    "off_count = train_dataset_df.loc[train_dataset_df['off_label'] == 'OFF'].shape[0]\n",
    "not_off_count = train_dataset_df.loc[train_dataset_df['off_label'] == 'NOT_OFF'].shape[0]\n",
    "num_samples = not_off_count - off_count\n",
    "off_aug_df = augment_text(train_dataset_df, aug_bert,label_name='off_label', label_val ='OFF', samples=num_samples, show=False)\n",
    "off_aug_df[\"tweet\"] = [string.replace(\"[UNK]\", \"\")  for string in off_aug_df['tweet']]\n",
    "off_aug_df['tweet'] = off_aug_df['tweet'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6969/6969 [1:07:21<00:00,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "#augment hate speech tweets\n",
    "hs_count = train_dataset_df.loc[train_dataset_df['hs_label'] == 'HS'].shape[0]\n",
    "not_hs_count = train_dataset_df.loc[train_dataset_df['hs_label'] == 'NOT_HS'].shape[0]\n",
    "num_samples = not_hs_count - hs_count\n",
    "hs_aug_df = augment_text(train_dataset_df, aug_bert, label_name='hs_label', label_val='HS', samples=num_samples, show=False)\n",
    "hs_aug_df['tweet'] = [string.replace(\"[UNK]\", \"\") for string in hs_aug_df['tweet']]\n",
    "hs_aug_df['tweet'] = hs_aug_df['tweet'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 15:01:54.391504: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-04-13 15:01:54.391631: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: kirollos-G3-3500\n",
      "2022-04-13 15:01:54.391666: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: kirollos-G3-3500\n",
      "2022-04-13 15:01:54.404566: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 510.47.3\n",
      "2022-04-13 15:01:54.404710: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 510.47.3\n",
      "2022-04-13 15:01:54.404747: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 510.47.3\n",
      "2022-04-13 15:01:54.406387: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-13 15:01:54.830154: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 307200000 exceeds 10% of free system memory.\n",
      "2022-04-13 15:01:55.448030: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 307200000 exceeds 10% of free system memory.\n",
      "2022-04-13 15:01:55.663415: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 307200000 exceeds 10% of free system memory.\n",
      "2022-04-13 15:02:10.193359: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 307200000 exceeds 10% of free system memory.\n",
      "2022-04-13 15:02:10.340514: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 307200000 exceeds 10% of free system memory.\n",
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at UBC-NLP/MARBERT.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "marbert_model_path = 'UBC-NLP/MARBERT'\n",
    "tokenizer = AutoTokenizer.from_pretrained(marbert_model_path, from_tf=True)\n",
    "marbert_model = TFAutoModel.from_pretrained(marbert_model_path, output_hidden_states=True)\n",
    "\n",
    "def bert_tokenize(texts: str) -> list:\n",
    "    max_len = 0\n",
    "    for text in texts:\n",
    "        max_len = max(len(tokenizer.tokenize(f'[CLS] {text} [SEP]')), max_len)\n",
    "    tokens = tokenizer(texts, padding='max_length', truncation=True, max_length=max_len)\n",
    "    return (tokens['input_ids'], tokens['attention_mask'], tokens['token_type_ids'])\n",
    "\n",
    "def get_embeddings(tokens):\n",
    "    ids = tf.convert_to_tensor(tokens[0])\n",
    "    mask = tf.convert_to_tensor(tokens[1])\n",
    "    type_ids = tf.convert_to_tensor(tokens[2])\n",
    "    hidden_states = marbert_model(input_ids=ids, attention_mask=mask, token_type_ids=type_ids)[2]\n",
    "    sentence_embd = tf.reduce_mean(tf.reduce_sum(tf.stack(hidden_states[-4:]), axis = 0), axis=1)\n",
    "    return sentence_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11430,)\n",
      "int32\n"
     ]
    }
   ],
   "source": [
    "#prepare offensive task labels\n",
    "off_aug_df.loc[off_aug_df['off_label'] == 'OFF', 'off_label'] = 1\n",
    "off_aug_df.loc[off_aug_df['off_label'] == 'NOT_OFF', 'off_label'] = 0\n",
    "del off_aug_df['hs_label']\n",
    "off_aug_df['off_label'] = off_aug_df['off_label'].astype(np.int32)\n",
    "y_train_offensive = off_aug_df['off_label'].values\n",
    "print(y_train_offensive.shape)\n",
    "print(y_train_offensive.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract training features for hate speech task\n",
    "text_offensive = off_aug_df['tweet'].values \n",
    "batch_size = 64\n",
    "train_num_batches = text_offensive.shape[0] // batch_size\n",
    "train_rem = text_offensive.shape[0] % batch_size\n",
    "\n",
    "x_train_offensive = np.empty(shape=(text_offensive.shape[0], 768), dtype=np.float64)\n",
    "for batch_step in tqdm(range(1, train_num_batches), desc='getting training features..'):\n",
    "    i = batch_step - 1\n",
    "    tokens = bert_tokenize(list(text_offensive[i * batch_size: batch_step * batch_size]))\n",
    "    x_train_offensive[i * batch_size : batch_step * batch_size] = get_embeddings(tokens)\n",
    "\n",
    "if train_rem != 0:\n",
    "    tokens = bert_tokenize(list(text_offensive[text_offensive.shape[0] - train_rem: ]))\n",
    "    x_train_offensive[text_offensive.shape[0] - train_rem: ] = get_embeddings(tokens)\n",
    "\n",
    "print(x_train_offensive.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15856,)\n",
      "int32\n"
     ]
    }
   ],
   "source": [
    "#prepare hate speech task labels\n",
    "hs_aug_df.loc[hs_aug_df['hs_label'] == 'HS', 'hs_label'] = 1\n",
    "hs_aug_df.loc[hs_aug_df['hs_label'] == 'NOT_HS', 'hs_label'] = 0\n",
    "del hs_aug_df['off_label']\n",
    "hs_aug_df['hs_label'] = hs_aug_df['hs_label'].astype(np.int32)\n",
    "y_train_hs = hs_aug_df['hs_label'].values\n",
    "print(y_train_hs.shape)\n",
    "print(y_train_hs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "getting training features..: 100%|██████████| 246/246 [30:39<00:00,  7.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15856, 768)\n"
     ]
    }
   ],
   "source": [
    "# extract training features for hate speech task\n",
    "text_hs = hs_aug_df['tweet'].values \n",
    "batch_size = 64\n",
    "train_num_batches = text_hs.shape[0] // batch_size\n",
    "train_rem = text_hs.shape[0] % batch_size\n",
    "\n",
    "x_train_hs = np.empty(shape=(text_hs.shape[0], 768), dtype=np.float64)\n",
    "for batch_step in tqdm(range(1, train_num_batches), desc='getting training features..'):\n",
    "    i = batch_step - 1\n",
    "    tokens = bert_tokenize(list(text_hs[i * batch_size: batch_step * batch_size]))\n",
    "    x_train_hs[i * batch_size : batch_step * batch_size] = get_embeddings(tokens)\n",
    "\n",
    "if train_rem != 0:\n",
    "    tokens = bert_tokenize(list(text_hs[text_hs.shape[0] - train_rem: ]))\n",
    "    x_train_hs[text_hs.shape[0] - train_rem: ] = get_embeddings(tokens)\n",
    "\n",
    "print(x_train_hs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read development dataset and prepare offensive and hate speech labels\n",
    "dev_dataset_df = pd.read_csv('./dataset/OSACT2022-sharedTask-dev.csv', usecols=['tweet','off_label', 'hs_label'])\n",
    "dev_dataset_df.loc[dev_dataset_df['off_label'] == 'OFF', 'off_label'] = 1\n",
    "dev_dataset_df.loc[dev_dataset_df['off_label'] == 'NOT_OFF', 'off_label'] = 0\n",
    "dev_dataset_df['off_label'] = dev_dataset_df['off_label'].astype(np.int32)\n",
    "y_dev_offensive = dev_dataset_df['off_label'].values\n",
    "\n",
    "dev_dataset_df.loc[dev_dataset_df['hs_label'] == 'HS1', 'hs_label'] = 1\n",
    "dev_dataset_df.loc[dev_dataset_df['hs_label'] == 'HS2', 'hs_label'] = 1\n",
    "dev_dataset_df.loc[dev_dataset_df['hs_label'] == 'HS3', 'hs_label'] = 1\n",
    "dev_dataset_df.loc[dev_dataset_df['hs_label'] == 'HS4', 'hs_label'] = 1\n",
    "dev_dataset_df.loc[dev_dataset_df['hs_label'] == 'HS5', 'hs_label'] = 1\n",
    "dev_dataset_df.loc[dev_dataset_df['hs_label'] == 'HS6', 'hs_label'] = 1\n",
    "dev_dataset_df.loc[(dev_dataset_df['hs_label'] == 'NOT_HS'), 'hs_label'] = 0\n",
    "dev_dataset_df['hs_label'] = dev_dataset_df['hs_label'].astype(np.int32)\n",
    "y_dev_hs = dev_dataset_df['hs_label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "getting dev features..: 100%|██████████| 18/18 [01:47<00:00,  5.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1270, 768)\n"
     ]
    }
   ],
   "source": [
    "#extract development dataset features\n",
    "text_offensive = dev_dataset_df['tweet'].values \n",
    "batch_size = 64\n",
    "dev_num_batches = text_offensive.shape[0] // batch_size\n",
    "dev_rem = text_offensive.shape[0] % batch_size\n",
    "\n",
    "x_dev = np.empty(shape=(text_offensive.shape[0], 768), dtype=np.float64)\n",
    "for batch_step in tqdm(range(1, dev_num_batches), desc='getting dev features..'):\n",
    "    i = batch_step - 1\n",
    "    tokens = bert_tokenize(list(text_offensive[i * batch_size: batch_step * batch_size]))\n",
    "    x_dev[i * batch_size : batch_step * batch_size] = get_embeddings(tokens)\n",
    "\n",
    "if dev_rem != 0:\n",
    "    tokens = bert_tokenize(list(text_offensive[text_offensive.shape[0] - dev_rem: ]))\n",
    "    x_dev[text_offensive.shape[0] - dev_rem: ] = get_embeddings(tokens)\n",
    "\n",
    "print(x_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.20669558\n",
      "Epoch 3, change: 0.10823747\n",
      "Epoch 4, change: 0.06977143\n",
      "Epoch 5, change: 0.04783693\n",
      "Epoch 6, change: 0.02658286\n",
      "Epoch 7, change: 0.01648776\n",
      "Epoch 8, change: 0.01105328\n",
      "Epoch 9, change: 0.00894443\n",
      "Epoch 10, change: 0.00493904\n",
      "Epoch 11, change: 0.00387966\n",
      "Epoch 12, change: 0.00325783\n",
      "Epoch 13, change: 0.00242496\n",
      "Epoch 14, change: 0.00170599\n",
      "Epoch 15, change: 0.00131166\n",
      "Epoch 16, change: 0.00113192\n",
      "Epoch 17, change: 0.00081929\n",
      "Epoch 18, change: 0.00065565\n",
      "Epoch 19, change: 0.00047070\n",
      "Epoch 20, change: 0.00039170\n",
      "Epoch 21, change: 0.00027360\n",
      "Epoch 22, change: 0.00021646\n",
      "Epoch 23, change: 0.00017753\n",
      "Epoch 24, change: 0.00012184\n",
      "Epoch 25, change: 0.00010307\n",
      "convergence after 26 epochs took 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.001, solver='saga', verbose=2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logistic regression model for offensive task\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_offensive = scaler.fit_transform(x_train_offensive, y_train_offensive)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train_offensive, y_train_offensive, test_size=0.3)\n",
    "\n",
    "offensive_lr_model = LogisticRegression(verbose=2, solver='saga', C=1e-3)\n",
    "\n",
    "offensive_lr_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate logistic regression on training, test and development datasets for offensive task\n",
    "x_dev_offensive = scaler.fit(x_dev)\n",
    "print('train data confusion matrix:')\n",
    "train_data_conf_matrix = classification_report(y_train, offensive_lr_model.predict(x_train), target_names=['not_off', 'off'])\n",
    "print(train_data_conf_matrix)\n",
    "print('test data confusion matrix:')\n",
    "test_data_conf_matrix = classification_report(y_test, offensive_lr_model.predict(x_test), target_names=['not_off', 'off'])\n",
    "print(test_data_conf_matrix)\n",
    "print('dev data confusion matrix:')\n",
    "dev_data_conf_matrix = classification_report(y_dev_offensive, offensive_lr_model.predict(x_dev_offensive), target_names=['not_off', 'off'])\n",
    "print(dev_data_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   47.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_samples=0.4, verbose=2)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random forest model for offensive task\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_offensive = scaler.fit_transform(x_train_offensive, y_train_offensive)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train_offensive, y_train_offensive, test_size=0.3)\n",
    "\n",
    "offensive_rf_model = RandomForestClassifier(verbose=2, max_samples=0.4)\n",
    "\n",
    "offensive_rf_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate random forest on training, test and development datasets for offensive task\n",
    "x_dev_offensive = scaler.fit(x_dev)\n",
    "print('train data confusion matrix:')\n",
    "train_data_conf_matrix = classification_report(y_train, offensive_rf_model.predict(x_train), target_names=['not_off', 'off'])\n",
    "print(train_data_conf_matrix)\n",
    "print('test data confusion matrix:')\n",
    "test_data_conf_matrix = classification_report(y_test, offensive_rf_model.predict(x_test), target_names=['not_off', 'off'])\n",
    "print(test_data_conf_matrix)\n",
    "print('dev data confusion matrix:')\n",
    "dev_data_conf_matrix = classification_report(y_dev_offensive, offensive_rf_model.predict(x_dev_offensive), target_names=['not_off', 'off'])\n",
    "print(dev_data_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, change: 1.00000000\n",
      "Epoch 2, change: 0.22245601\n",
      "Epoch 3, change: 0.13065225\n",
      "Epoch 4, change: 0.06529777\n",
      "Epoch 5, change: 0.04371365\n",
      "Epoch 6, change: 0.03118205\n",
      "Epoch 7, change: 0.01940550\n",
      "Epoch 8, change: 0.01335161\n",
      "Epoch 9, change: 0.01064205\n",
      "Epoch 10, change: 0.00771064\n",
      "Epoch 11, change: 0.00414935\n",
      "Epoch 12, change: 0.00285662\n",
      "Epoch 13, change: 0.00210780\n",
      "Epoch 14, change: 0.00152659\n",
      "Epoch 15, change: 0.00118896\n",
      "Epoch 16, change: 0.00093127\n",
      "Epoch 17, change: 0.00068015\n",
      "Epoch 18, change: 0.00054178\n",
      "Epoch 19, change: 0.00038536\n",
      "Epoch 20, change: 0.00028793\n",
      "Epoch 21, change: 0.00021724\n",
      "Epoch 22, change: 0.00016804\n",
      "Epoch 23, change: 0.00012574\n",
      "convergence after 24 epochs took 2 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.001, solver='saga', verbose=2)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logistic regression model for hate speech task\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_hs = scaler.fit_transform(x_train_hs, y_train_hs)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train_hs, y_train_hs, test_size=0.3)\n",
    "\n",
    "hs_lr_model = LogisticRegression(verbose=2, solver='saga', C=1e-3)\n",
    "\n",
    "hs_lr_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate logistic regression on training, test and development datasets for hate speech task\n",
    "x_dev_hs = scaler.fit(x_dev)\n",
    "print('train data confusion matrix:')\n",
    "train_data_conf_matrix = classification_report(y_train, hs_lr_model.predict(x_train), target_names=['not_hs', 'hs'])\n",
    "print(train_data_conf_matrix)\n",
    "print('test data confusion matrix:')\n",
    "test_data_conf_matrix = classification_report(y_test, hs_lr_model.predict(x_test), target_names=['not_hs', 'hs'])\n",
    "print(test_data_conf_matrix)\n",
    "print('dev data confusion matrix:')\n",
    "dev_data_conf_matrix = classification_report(y_dev_offensive, hs_lr_model.predict(x_dev_hs), target_names=['not_hs', 'hs'])\n",
    "print(dev_data_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   12.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_samples=0.4, verbose=2)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random forest model for hate speech task\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_hs = scaler.fit_transform(x_train_hs, y_train_hs)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train_hs, y_train_hs, test_size=0.3)\n",
    "\n",
    "hs_rf_model = RandomForestClassifier(verbose=2, max_samples=0.4)\n",
    "\n",
    "hs_rf_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate random forest on training, test and development datasets for hate speech task\n",
    "x_dev_hs = scaler.fit(x_dev)\n",
    "print('train data confusion matrix:')\n",
    "train_data_conf_matrix = classification_report(y_train, hs_rf_model.predict(x_train), target_names=['not_hs', 'hs'])\n",
    "print(train_data_conf_matrix)\n",
    "print('test data confusion matrix:')\n",
    "test_data_conf_matrix = classification_report(y_test, hs_rf_model.predict(x_test), target_names=['not_hs', 'hs'])\n",
    "print(test_data_conf_matrix)\n",
    "print('dev data confusion matrix:')\n",
    "dev_data_conf_matrix = classification_report(y_dev_offensive, hs_rf_model.predict(x_dev_hs), target_names=['not_hs', 'hs'])\n",
    "print(dev_data_conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
